{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls ~/trained_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from datatype_recovery.models.dataset import load_dataset_from_path, max_typesequence_len_in_dataset\n",
    "from datatype_recovery.models.dataset.encoding import *\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datatype_recovery.models.metrics import acc_heuristic_numcorrect\n",
    "\n",
    "def eval_model_on_dataset(model_path:Path, device:str, dataset_path:Path) -> float:\n",
    "    '''\n",
    "    Evaluates the model on the given dataset and returns the accuracy of the corrected\n",
    "    model output against the dataset labels\n",
    "    '''\n",
    "    dataset = load_dataset_from_path(dataset_path)\n",
    "    max_true_seq_len = max_typesequence_len_in_dataset(dataset_path)\n",
    "    return eval_model_on_subset(model_path, device, dataset, max_true_seq_len)\n",
    "\n",
    "def eval_model_on_subset(model_path:Path, device:str, dataset, max_true_seq_len:int) -> float:\n",
    "    '''\n",
    "    Evaluates the model on the given subset and returns the accuracy of the corrected\n",
    "    model output against the dataset labels\n",
    "    '''\n",
    "    model = torch.load(model_path)\n",
    "    print(model)\n",
    "\n",
    "    # take the max of model seq length and max seq length of dataset so we\n",
    "    # calculate accuracy correctly (without truncating something)\n",
    "    max_len = max(model.max_seq_len, max_true_seq_len)\n",
    "\n",
    "    # prepare the data loaders\n",
    "    batch_size = 64\n",
    "    dataset.transform = T.Compose([ToBatchTensors(), ToFixedLengthTypeSeq(max_len)])\n",
    "\n",
    "    # split the dataset into the part divisible by batch size and the leftovers\n",
    "    # we can chain these together for performance - our metrics simply iterate\n",
    "    # through all elements in the batch\n",
    "    batched_total = len(dataset)-(len(dataset)%batch_size)\n",
    "    batch_loader = DataLoader(dataset[:batched_total], batch_size=batch_size)\n",
    "    leftovers_loader = DataLoader(dataset[batched_total:], batch_size=1)\n",
    "\n",
    "    print(f'Running eval...')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "\n",
    "    for data in tqdm(chain(batch_loader, leftovers_loader), total=len(batch_loader)+len(leftovers_loader)):\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        num_correct += acc_heuristic_numcorrect(data.y, out)\n",
    "\n",
    "    accuracy = num_correct/len(dataset)\n",
    "    print(f'Accuracy = {accuracy*100:,.2f}%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: eval on an interesting subset of the data\n",
    "# from torch.utils.data import Subset\n",
    "# Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating max true sequence length in dataset trainset_astera_5hops_nocomp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24691/24691 [00:19<00:00, 1253.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructuralTypeSeqModel(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): GATConv(31, 128, heads=1)\n",
      "    (1-2): 2 x GATConv(128, 128, heads=1)\n",
      "  )\n",
      "  (pred_head): Linear(128, 66, bias=True)\n",
      ")\n",
      "Running eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:17<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 2.22%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022234822404924873"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path.home()/'trainset_astera_5hops_nocomp'\n",
    "model_path = Path.home()/'trained_models/structural_3out_3hops_nocomp.pt'\n",
    "\n",
    "eval_model_on_dataset(model_path, 'cuda:3', dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_path(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1323424, '17', 'l')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].varid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pd.read_csv(dataset.root/dataset.raw_file_names[0])\n",
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "ds = TypeSequenceDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RunGid</th>\n",
       "      <th>RunFolder</th>\n",
       "      <th>BinariesCsv</th>\n",
       "      <th>FuncsCsv</th>\n",
       "      <th>ParamsCsv</th>\n",
       "      <th>LocalsCsv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/run1</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RunGid                                         RunFolder   \n",
       "0       0  /home/cls0027/exp_builds/astera.exp/rundata/run1  \\\n",
       "\n",
       "                                         BinariesCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                            FuncsCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                           ParamsCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                           LocalsCsv  \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runs = pd.read_csv(ds.exp_runs_path)\n",
    "exp_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: to slice and dice this dataset, we really need to UNIFY the data frames (binaries/funcs/params/locals)\n",
    "# (NOTE this should actually happen during initial dataset creation, unless we specify a --quick mode or something)\n",
    "#\n",
    "# we HAVE RunGid mapping (RunGID->files)\n",
    "#\n",
    "# --> convert (local) binary IDs to a global binary ID (binary GID)\n",
    "# (find # binaries in each run, take the max # binaries and round up to the NEXT largest 1,000...this is the BASE binary GID for this run)\n",
    "# (now add each binary ID to the BASE to get its GID, for all tables (bins/funcs/params/locals))\n",
    "# (save the base in this exp_runs table)\n",
    "#       --> HAVE TO COPY THE FILES LOCALLY during this step so we don't overwrite the originals!\n",
    "# --> pd.concat() all tables across all runs for each table type, one at a time (binary GIDs should now be unique)\n",
    "\n",
    "# ...NOW we should be able to run our analysis to slice/dice the data based on the\n",
    "# whole dataset (split/index using the varid's for individual data points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
