{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls ~/trained_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from datatype_recovery.models.dataset import load_dataset_from_path, max_typesequence_len_in_dataset\n",
    "from datatype_recovery.models.dataset.encoding import *\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datatype_recovery.models.metrics import acc_heuristic_numcorrect\n",
    "\n",
    "def eval_model_on_dataset(model_path:Path, device:str, dataset_path:Path) -> float:\n",
    "    '''\n",
    "    Evaluates the model on the given dataset and returns the accuracy of the corrected\n",
    "    model output against the dataset labels\n",
    "    '''\n",
    "    dataset = load_dataset_from_path(dataset_path)\n",
    "    max_true_seq_len = max_typesequence_len_in_dataset(dataset_path)\n",
    "    return eval_model_on_subset(model_path, device, dataset, max_true_seq_len)\n",
    "\n",
    "def eval_model_on_subset(model_path:Path, device:str, dataset, max_true_seq_len:int) -> float:\n",
    "    '''\n",
    "    Evaluates the model on the given subset and returns the accuracy of the corrected\n",
    "    model output against the dataset labels\n",
    "    '''\n",
    "    model = torch.load(model_path)\n",
    "    print(model)\n",
    "\n",
    "    # take the max of model seq length and max seq length of dataset so we\n",
    "    # calculate accuracy correctly (without truncating something)\n",
    "    max_len = max(model.max_seq_len, max_true_seq_len)\n",
    "\n",
    "    # prepare the data loaders\n",
    "    batch_size = 64\n",
    "    dataset.transform = T.Compose([ToBatchTensors(), ToFixedLengthTypeSeq(max_len)])\n",
    "\n",
    "    # split the dataset into the part divisible by batch size and the leftovers\n",
    "    # we can chain these together for performance - our metrics simply iterate\n",
    "    # through all elements in the batch\n",
    "    batched_total = len(dataset)-(len(dataset)%batch_size)\n",
    "    batch_loader = DataLoader(dataset[:batched_total], batch_size=batch_size)\n",
    "    leftovers_loader = DataLoader(dataset[batched_total:], batch_size=1)\n",
    "\n",
    "    print(f'Running eval...')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "\n",
    "    for data in tqdm(chain(batch_loader, leftovers_loader), total=len(batch_loader)+len(leftovers_loader)):\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        num_correct += acc_heuristic_numcorrect(data.y, out)\n",
    "\n",
    "    accuracy = num_correct/len(dataset)\n",
    "    print(f'Accuracy = {accuracy*100:,.2f}%')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: eval on an interesting subset of the data\n",
    "# from torch.utils.data import Subset\n",
    "# Subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating max true sequence length in dataset trainset_astera_5hops_nocomp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24691/24691 [00:20<00:00, 1184.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructuralTypeSeqModel(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): GATConv(31, 128, heads=1)\n",
      "    (1-2): 2 x GATConv(128, 128, heads=1)\n",
      "  )\n",
      "  (pred_head): Linear(128, 66, bias=True)\n",
      ")\n",
      "Running eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:17<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 2.22%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022234822404924873"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = Path.home()/'trainset_astera_5hops_nocomp'\n",
    "model_path = Path.home()/'trained_models/structural_3out_3hops_nocomp.pt'\n",
    "\n",
    "eval_model_on_dataset(model_path, 'cuda:3', dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_from_path(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1323424, '17', 'l')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].varid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pd.read_csv(dataset.root/dataset.raw_file_names[0])\n",
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "ds = TypeSequenceDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RunGid</th>\n",
       "      <th>RunFolder</th>\n",
       "      <th>BinariesCsv</th>\n",
       "      <th>FuncsCsv</th>\n",
       "      <th>ParamsCsv</th>\n",
       "      <th>LocalsCsv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/run1</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "      <td>/home/cls0027/exp_builds/astera.exp/rundata/ru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RunGid                                         RunFolder   \n",
       "0       0  /home/cls0027/exp_builds/astera.exp/rundata/run1  \\\n",
       "\n",
       "                                         BinariesCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                            FuncsCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                           ParamsCsv   \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  \\\n",
       "\n",
       "                                           LocalsCsv  \n",
       "0  /home/cls0027/exp_builds/astera.exp/rundata/ru...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runs = pd.read_csv(ds.exp_runs_path)\n",
    "exp_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: to slice and dice this dataset, we really need to UNIFY the data frames (binaries/funcs/params/locals)\n",
    "# (NOTE this should actually happen during initial dataset creation, unless we specify a --quick mode or something)\n",
    "#\n",
    "# we HAVE RunGid mapping (RunGID->files)\n",
    "#\n",
    "# --> convert (local) binary IDs to a global binary ID (binary GID)\n",
    "# (find # binaries in each run, take the max # binaries and round up to the NEXT largest 1,000...this is the BASE binary GID for this run)\n",
    "# (now add each binary ID to the BASE to get its GID, for all tables (bins/funcs/params/locals))\n",
    "# (save the base in this exp_runs table)\n",
    "#       --> HAVE TO COPY THE FILES LOCALLY during this step so we don't overwrite the originals!\n",
    "# --> pd.concat() all tables across all runs for each table type, one at a time (binary GIDs should now be unique)\n",
    "\n",
    "# ...NOW we should be able to run our analysis to slice/dice the data based on the\n",
    "# whole dataset (split/index using the varid's for individual data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BinaryId</th>\n",
       "      <th>Name</th>\n",
       "      <th>OrigBinaryId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>fighter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>assets</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>config</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>audio</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>input</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1005</td>\n",
       "      <td>debug</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1006</td>\n",
       "      <td>sprites</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1007</td>\n",
       "      <td>collision</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1008</td>\n",
       "      <td>pakutil</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BinaryId       Name  OrigBinaryId\n",
       "0      1000    fighter             0\n",
       "1      1001     assets             1\n",
       "2      1002     config             2\n",
       "3      1003      audio             3\n",
       "4      1004      input             4\n",
       "5      1005      debug             5\n",
       "6      1006    sprites             6\n",
       "7      1007  collision             7\n",
       "8      1008    pakutil             8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: do this for all projects in the dataset...\n",
    "\n",
    "base_gid = 1000     # start here\n",
    "\n",
    "for i in range(len(exp_runs)):\n",
    "    bins_df = pd.read_csv(exp_runs.iloc[i].BinariesCsv)\n",
    "    next_base_gid = base_gid + int(len(bins_df)/1000)*1000 + 1000\n",
    "\n",
    "    bins_df['OrigBinaryId'] = bins_df['BinaryId']\n",
    "    bins_df['BinaryId'] = bins_df.BinaryId + base_gid\n",
    "    bins_df\n",
    "\n",
    "    base_gid = next_base_gid    # update for next entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs_df = pd.read_csv(exp_runs.iloc[0].FuncsCsv)\n",
    "params_df = pd.read_csv(exp_runs.iloc[0].ParamsCsv)\n",
    "locals_df = pd.read_csv(exp_runs.iloc[0].LocalsCsv)\n",
    "\n",
    "df_list = [funcs_df, params_df, locals_df]\n",
    "\n",
    "for df in df_list:\n",
    "    df['BinaryId'] = df.BinaryId.apply(lambda bid: bins_df[bins_df.OrigBinaryId==bid].BinaryId.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: concat all funcs_dfs, params_dfs, etc. before writing to dataset root/processed/params.csv\n",
    "# TODO: write all 4 files back to local root folder (root/processed I guess)\n",
    "# TODO: have the VariableGraphBuilder use these files while creating the dataset instead of the original files\n",
    "# - download copies files locally if desired\n",
    "# - process FIRST does this logic (remap binaries, combine dfs, write to csv) then\n",
    "#   creates var graphs from THESE csvs (so varid will have global binid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunGid                                                         0\n",
       "RunFolder       /home/cls0027/exp_builds/astera.exp/rundata/run1\n",
       "BinariesCsv    /home/cls0027/exp_builds/astera.exp/rundata/ru...\n",
       "FuncsCsv       /home/cls0027/exp_builds/astera.exp/rundata/ru...\n",
       "ParamsCsv      /home/cls0027/exp_builds/astera.exp/rundata/ru...\n",
       "LocalsCsv      /home/cls0027/exp_builds/astera.exp/rundata/ru...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runs.iloc[0].RunFolder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
