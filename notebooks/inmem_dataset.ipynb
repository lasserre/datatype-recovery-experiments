{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input_params dict supplied but saved .json file also found\n",
      "input_params will be IGNORED in favor of saved .json file (trainset_astera/raw/input_params.json)\n"
     ]
    }
   ],
   "source": [
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "# COPIED for testing...\n",
    "MAX_HOPS = 3\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/exp_builds/astera.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "\n",
    "# NOTE:\n",
    "# - 4096 is NOT that much more than 1024 which ran pretty fast...\n",
    "#       >> my \"batchsize\" (# data points saved in one chunk file) is 1000...just under 1024\n",
    "#\n",
    "#       >>>>>> I ALSO DID NOT SHUFFLE THE DATASET BEFORE!! <<<<<<\n",
    "#\n",
    "# So not only was the dataset reading from files, but also after shuffling I think\n",
    "# it was THRASHING back and forth across completely separate files...\n",
    "# - since it is a not-in-memory dataset it can't assume everything fits...probably\n",
    "#   read in each file froms scratch every time!!\n",
    "\n",
    "# > so we could increase batchsize too...\n",
    "\n",
    "dataset = TypeSequenceDataset('trainset_astera', data_params, max_hops=MAX_HOPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing_finished']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.root\n",
    "dataset.raw_file_names\n",
    "dataset.processed_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "import tqdm\n",
    "from tqdm.auto import trange\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO: put this (dup) file IN the same folder...for now\n",
    "\n",
    "class InMemTypeSequenceDataset(InMemoryDataset):\n",
    "    '''\n",
    "    For datasets that fit, use an in-memory dataset for HUGE performance boost!!\n",
    "    '''\n",
    "    def __init__(self, dataset:TypeSequenceDataset):\n",
    "        self.src_dataset = dataset\n",
    "\n",
    "        super().__init__(dataset.root,\n",
    "            transform=dataset.transform,\n",
    "            pre_transform=dataset.pre_transform,\n",
    "            pre_filter=dataset.pre_filter)\n",
    "\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # we need these to be here before we can copy to our version\n",
    "        return self.src_dataset.raw_file_names\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['IN_MEMORY_COPY.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`\n",
    "        print(f'Looks like the raw source dataset is missing for: {self.src_dataset}')\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        for fname in self.src_dataset.processed_file_names:\n",
    "            if not (Path(self.root)/'processed'/fname).exists():\n",
    "                raise Exception(f'Looks like the raw source dataset is missing for: {self.src_dataset}')\n",
    "\n",
    "        folder_size_str = subprocess.check_output(f'du -ch {self.src_dataset.root} | tail -1',\n",
    "            shell=True).decode('utf-8').split()[0]\n",
    "        print(f'Loading dataset into memory of size {folder_size_str}')\n",
    "\n",
    "        # data_list = list(self.src_dataset)      # we assume it fits in memory...lol\n",
    "        ds_len = len(self.src_dataset)\n",
    "        data_list = [self.src_dataset[i] for i in trange(ds_len)]\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset into memory of size 149M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28f9d5e3f64f258fa76913d0ea3f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "in_mem = InMemTypeSequenceDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e09eca1e2e44628b1e60fdca24b74de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_list = [in_mem[i] for i in trange(len(in_mem))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'269M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "path = 'trainset_astera'\n",
    "folder_size_str = subprocess.check_output(f'du -ch {path} | tail -1', shell=True).decode('utf-8').split()[0]\n",
    "# num_bytes/2**20\n",
    "folder_size_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12K\ttrainset_astera/raw\n",
      "269M\ttrainset_astera/processed\n",
      "269M\ttrainset_astera\n",
      "269M\ttotal\n"
     ]
    }
   ],
   "source": [
    "!du -ch trainset_astera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: du [OPTION]... [FILE]...\n",
      "  or:  du [OPTION]... --files0-from=F\n",
      "Summarize disk usage of the set of FILEs, recursively for directories.\n",
      "\n",
      "Mandatory arguments to long options are mandatory for short options too.\n",
      "  -0, --null            end each output line with NUL, not newline\n",
      "  -a, --all             write counts for all files, not just directories\n",
      "      --apparent-size   print apparent sizes, rather than disk usage; although\n",
      "                          the apparent size is usually smaller, it may be\n",
      "                          larger due to holes in ('sparse') files, internal\n",
      "                          fragmentation, indirect blocks, and the like\n",
      "  -B, --block-size=SIZE  scale sizes by SIZE before printing them; e.g.,\n",
      "                           '-BM' prints sizes in units of 1,048,576 bytes;\n",
      "                           see SIZE format below\n",
      "  -b, --bytes           equivalent to '--apparent-size --block-size=1'\n",
      "  -c, --total           produce a grand total\n",
      "  -D, --dereference-args  dereference only symlinks that are listed on the\n",
      "                          command line\n",
      "  -d, --max-depth=N     print the total for a directory (or file, with --all)\n",
      "                          only if it is N or fewer levels below the command\n",
      "                          line argument;  --max-depth=0 is the same as\n",
      "                          --summarize\n",
      "      --files0-from=F   summarize disk usage of the\n",
      "                          NUL-terminated file names specified in file F;\n",
      "                          if F is -, then read names from standard input\n",
      "  -H                    equivalent to --dereference-args (-D)\n",
      "  -h, --human-readable  print sizes in human readable format (e.g., 1K 234M 2G)\n",
      "      --inodes          list inode usage information instead of block usage\n",
      "  -k                    like --block-size=1K\n",
      "  -L, --dereference     dereference all symbolic links\n",
      "  -l, --count-links     count sizes many times if hard linked\n",
      "  -m                    like --block-size=1M\n",
      "  -P, --no-dereference  don't follow any symbolic links (this is the default)\n",
      "  -S, --separate-dirs   for directories do not include size of subdirectories\n",
      "      --si              like -h, but use powers of 1000 not 1024\n",
      "  -s, --summarize       display only a total for each argument\n",
      "  -t, --threshold=SIZE  exclude entries smaller than SIZE if positive,\n",
      "                          or entries greater than SIZE if negative\n",
      "      --time            show time of the last modification of any file in the\n",
      "                          directory, or any of its subdirectories\n",
      "      --time=WORD       show time as WORD instead of modification time:\n",
      "                          atime, access, use, ctime or status\n",
      "      --time-style=STYLE  show times using STYLE, which can be:\n",
      "                            full-iso, long-iso, iso, or +FORMAT;\n",
      "                            FORMAT is interpreted like in 'date'\n",
      "  -X, --exclude-from=FILE  exclude files that match any pattern in FILE\n",
      "      --exclude=PATTERN    exclude files that match PATTERN\n",
      "  -x, --one-file-system    skip directories on different file systems\n",
      "      --help     display this help and exit\n",
      "      --version  output version information and exit\n",
      "\n",
      "Display values are in units of the first available SIZE from --block-size,\n",
      "and the DU_BLOCK_SIZE, BLOCK_SIZE and BLOCKSIZE environment variables.\n",
      "Otherwise, units default to 1024 bytes (or 512 if POSIXLY_CORRECT is set).\n",
      "\n",
      "The SIZE argument is an integer and optional unit (example: 10K is 10*1024).\n",
      "Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\n",
      "\n",
      "GNU coreutils online help: <https://www.gnu.org/software/coreutils/>\n",
      "Full documentation at: <https://www.gnu.org/software/coreutils/du>\n",
      "or available locally via: info '(coreutils) du invocation'\n"
     ]
    }
   ],
   "source": [
    "!du --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
