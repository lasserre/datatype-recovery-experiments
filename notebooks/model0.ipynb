{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "> PICK UP HERE: **implement model end-to-end ASAP using Dataset, saving model state every N epochs, etc.**\n",
    "- See todo list in OneNote\n",
    "- Once working, make the saving model state logic generic so I can use with any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input_params dict supplied but saved .json file also found\n",
      "input_params will be IGNORED in favor of saved .json file (model0_dataset/raw/input_params.json)\n"
     ]
    }
   ],
   "source": [
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/test_builds/astera.exp/rundata/run1',\n",
    "        '/home/cls0027/test_builds/coreutils.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "dataset = TypeSequenceDataset('model0_dataset', data_params, max_hops=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split test/train on BINARY boundary to prevent functions/vars from crossing\n",
    "# -> I have varid tuples, so I can get (rungid, binid) from that and use it to split\n",
    "#   - extract (idx, rungid, binid) for each data point and put into a dataframe\n",
    "#   - groupby/count dataframe by (rungid/binid) and come up with split that is close to target %\n",
    "#   - collect indices for each data point within each subset (test/train/valid)\n",
    "#   - quick/simple validate that all (rungid/binid) combos are unique to a subset\n",
    "#\n",
    "# [(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# tiny subset to start\n",
    "trainset_indices = list(range(12))\n",
    "testset_indices = list(range(100,112))\n",
    "\n",
    "train_set = Subset(dataset, trainset_indices)\n",
    "test_set = Subset(dataset, testset_indices)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# EXAMPLE: the shuffled slice changes each time, but dataset[0] stays consistent\n",
    "# print(dataset[:10].shuffle()[0].varid)\n",
    "# print(dataset[0].varid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[47, 47], edge_index=[2, 94], y=[4, 20], varid=[4], batch=[47], ptr=[5])\n",
      "\n",
      "Step 2\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[86, 47], edge_index=[2, 164], y=[4, 20], varid=[4], batch=[86], ptr=[5])\n",
      "\n",
      "Step 3\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[67, 47], edge_index=[2, 130], y=[4, 20], varid=[4], batch=[67], ptr=[5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step+1}')\n",
    "    print(f'Num graphs in batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44021"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TEMP: just split randomly for now\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 81.0), (1, 81.0), (1, 81.0), (1, 81.0), (1, 82.0)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rungid, bid, _, _, _ = dataset[0].varid\n",
    "[(x.varid[0], x.varid[1]) for x in dataset[10000:10100]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "indices = [10000, 10001, 10002, 10003, 10004]\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "ss = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(x.varid[0], x.varid[1]) for x in ss.shuffle()]\n",
    "loader = DataLoader(ss, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 81.0), (1, 82.0), (1, 81.0), (1, 81.0), (1, 81.0)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.varid[0][0], x.varid[0][1]) for x in loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.0\n",
       "1          0.0\n",
       "2          0.0\n",
       "3          0.0\n",
       "4          0.0\n",
       "         ...  \n",
       "11580    116.0\n",
       "11581    116.0\n",
       "11582    116.0\n",
       "11583    116.0\n",
       "11584    116.0\n",
       "Name: BinaryId, Length: 11585, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/binaries.csv')).BinaryId\n",
    "df = pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/locals.csv'))\n",
    "# df.BinaryId = df.BinaryId.astype('int64')\n",
    "# df.loc[:,'NEW_COL'] = pd.Series([1e3]*len(df), dtype='int64')\n",
    "df.BinaryId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1000\n",
       "1        1000\n",
       "2        1000\n",
       "3        1000\n",
       "4        1000\n",
       "         ... \n",
       "11580    1000\n",
       "11581    1000\n",
       "11582    1000\n",
       "11583    1000\n",
       "11584    1000\n",
       "Name: NEW_COL, Length: 11585, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.NEW_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      47\n",
       "1      47\n",
       "2      47\n",
       "3      47\n",
       "4      47\n",
       "       ..\n",
       "101    47\n",
       "102    47\n",
       "103    47\n",
       "104    47\n",
       "105    47\n",
       "Name: BinaryId, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/47.pathchk/functions.csv'))\n",
    "df.BinaryId"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
