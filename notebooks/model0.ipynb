{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "> PICK UP HERE: **implement model end-to-end ASAP using Dataset, saving model state every N epochs, etc.**\n",
    "- See todo list in OneNote\n",
    "- Once working, make the saving model state logic generic so I can use with any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input_params dict supplied but saved .json file also found\n",
      "input_params will be IGNORED in favor of saved .json file (model0_dataset/raw/input_params.json)\n"
     ]
    }
   ],
   "source": [
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/test_builds/astera.exp/rundata/run1',\n",
    "        '/home/cls0027/test_builds/coreutils.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "dataset = TypeSequenceDataset('model0_dataset', data_params, max_hops=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 640 samples (64.00%)\n",
      "Test set: 320 samples (32.00%)\n",
      "Batch size: 64\n",
      "Total usable dataset size (batch-aligned): 960\n",
      "Loss due to batch alignment: 40 (4.00%)\n"
     ]
    }
   ],
   "source": [
    "# TODO: split test/train on BINARY boundary to prevent functions/vars from crossing\n",
    "# -> I have varid tuples, so I can get (rungid, binid) from that and use it to split\n",
    "#   - extract (idx, rungid, binid) for each data point and put into a dataframe\n",
    "#   - groupby/count dataframe by (rungid/binid) and come up with split that is close to target %\n",
    "#   - collect indices for each data point within each subset (test/train/valid)\n",
    "#   - quick/simple validate that all (rungid/binid) combos are unique to a subset\n",
    "#\n",
    "# [(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "# trainset_indices = list(range(12))\n",
    "# testset_indices = list(range(100,112))\n",
    "# train_set = Subset(dataset, trainset_indices)\n",
    "# test_set = Subset(dataset, testset_indices)\n",
    "\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# tiny subset to start\n",
    "TRAIN_SPLIT = 0.7\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = False\n",
    "PIN_MEMORY=True\n",
    "\n",
    "dataset = dataset[:1000]\n",
    "\n",
    "# divide into train/test sets\n",
    "train_size = int(len(dataset)*TRAIN_SPLIT)\n",
    "train_size -= train_size%BATCH_SIZE     # align to batch size\n",
    "\n",
    "test_size = len(dataset) - train_size   # take the rest of the dataset...\n",
    "test_size -= test_size % BATCH_SIZE     # align to batch size\n",
    "\n",
    "train_set = Subset(dataset, range(0, 0+train_size))\n",
    "test_set = Subset(dataset, range(len(train_set), len(train_set)+test_size))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# EXAMPLE: the shuffled slice changes each time, but dataset[0] stays consistent\n",
    "# print(dataset[:10].shuffle()[0].varid)\n",
    "# print(dataset[0].varid)\n",
    "total_usable = len(train_set)+len(test_set)\n",
    "non_batch_aligned = len(dataset)-total_usable\n",
    "print(f'Train set: {len(train_set):,} samples ({len(train_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Test set: {len(test_set):,} samples ({len(test_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Total usable dataset size (batch-aligned): {total_usable:,}')\n",
    "print(f'Loss due to batch alignment: {non_batch_aligned:,} ({non_batch_aligned/len(dataset)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, data in enumerate(train_loader):\n",
    "#     if step > 5:\n",
    "#         break\n",
    "#     print(f'Step {step+1}')\n",
    "#     print(f'Num graphs in batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print(data.batch)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Batches\n",
    "This code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or more elegantly...\n",
    "\n",
    "# b = list(test_loader)[0]\n",
    "# batch_graphs = [(b.batch==i).nonzero(as_tuple=True) for i in range(BATCH_SIZE)]\n",
    "\n",
    "# node0_indices = [x[0][0].item() for x in batch_graphs]\n",
    "# node0_indices\n",
    "# batch_graphs\n",
    "# batch shape: [76, 47]\n",
    "# - 76 = sum of nodes in all graphs within the batch = 11+21+17+27\n",
    "# - 47 = # node features\n",
    "# batch_graphs\n",
    "# list of tensors, one tensor per graph within the batch\n",
    "# --> list of node_indexes, one per graph within the batch\n",
    "# each tensor holds a list of node indices for the nodes of that graph\n",
    "# (the indices index into b.x)\n",
    "# indices of nodes\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data.batch import DataBatch\n",
    "from typing import List\n",
    "\n",
    "def split_node_index_by_graph(batch:torch.tensor, batch_size:int) -> List[torch.tensor]:\n",
    "    '''\n",
    "    Returns the node_index for each individual graph within the batch as a list of tensors.\n",
    "\n",
    "    Each list entry (node_index) is a tensor of node indices for an individual graph within the\n",
    "    batch, where each of the node indices will index into the containing DataBatch.x.\n",
    "    '''\n",
    "    return [(batch==i).nonzero(as_tuple=True) for i in range(batch_size)]\n",
    "\n",
    "\n",
    "# c = list(loader2)[:4]\n",
    "# for x in list(test_loader)[:4]:\n",
    "    # print(x)\n",
    "\n",
    "# len(b.batch)\n",
    "# list(loader2)[3].edge_index\n",
    "# b.edge_index\n",
    "# b.batch[(b.batch==0).nonzero(as_tuple=True)]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# TODO: ok, here is the way to get node 0 indices\n",
    "# - edge_index should be able to take care of itself - we just pass it through, I\n",
    "#   don't think we need to do anything to it (it's correct, but won't have edges\n",
    "#   going between independent graphs of course)\n",
    "# ------------------------\n",
    "# TODO: PICK UP HERE and IMPLEMENT THE MODEL!!!\n",
    "# ------------------------\n",
    "# - use node0_indices to pull target node embeddings, passing into final layer (linear for now?)\n",
    "# - return prediction...finish building training function\n",
    "# - INTEGRATE WANDB!!\n",
    "# ------------------------\n",
    "\n",
    "def get_node0_indices(batch:torch.tensor) -> List[int]:\n",
    "    '''\n",
    "    Returns the indices of all \"node 0\" nodes in the batch\n",
    "    '''\n",
    "    batch_size = batch.max().item()+1\n",
    "    node_index_by_graph = split_node_index_by_graph(batch, batch_size)\n",
    "    return [x[0][0].item() for x in node_index_by_graph]\n",
    "\n",
    "# node_index_by_graph = split_node_index_by_graph(b.batch, BATCH_SIZE)\n",
    "# node0_indices = [x[0][0].item() for x in node_index_by_graph]\n",
    "# node0_indices\n",
    "# get_node0_indices(b.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently we can just index into x directly with these indices...\n",
    "# b.x[node0_indices].shape\n",
    "# for n0 in b.x[node0_indices]:\n",
    "#     print(decode_astnode(n0))\n",
    "\n",
    "# decode_typeseq(b.y)\n",
    "# decode_typeseq(b.y[None,0,:])\n",
    "# # dataset.num_classes\n",
    "# b.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.nn import GATConv, Linear\n",
    "\n",
    "class TypeSeqModel(torch.nn.Module):\n",
    "    def __init__(self, dataset:Dataset, hidden_channels:int):\n",
    "        super(TypeSeqModel, self).__init__()\n",
    "\n",
    "        # if we go with fewer layers than the # hops in our dataset\n",
    "        # that may be fine for experimenting, but eventually we are wasting\n",
    "        # time/space and can cut our dataset down to match (# hops = # layers)\n",
    "        self.gat1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.gat3 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # TODO - later on, add sequential layer(s) here\n",
    "\n",
    "        self.pred_head = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        node0_indices = get_node0_indices(batch)\n",
    "\n",
    "        # FIXME: if we need floats in the dataset, go back and save the data\n",
    "        # this way in our Dataset class\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # GNN layers\n",
    "        # ----------\n",
    "        # NOTE: it's tempting to downselect to node0 indices here, but I think\n",
    "        # that may be incorrect - we would not be passing ALL NODES through the\n",
    "        # network, just node 0.\n",
    "\n",
    "        # I think we WANT to compute the network on every node\n",
    "        # for N hops and then simply make predictions based on the node 0 nodes\n",
    "        #\n",
    "        # yeah...if we did not compute this on ALL nodes, I think we are\n",
    "        # making it effectively 1 hop only, and just going multiple rounds with 1 hop!\n",
    "\n",
    "        h = self.gat1(x, edge_index)\n",
    "        h = h.relu()\n",
    "\n",
    "        h = self.gat2(h, edge_index)\n",
    "        h = h.relu()\n",
    "\n",
    "        h = self.gat3(h, edge_index)\n",
    "        # relu here?\n",
    "\n",
    "        logits = self.pred_head(h[node0_indices])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeSeqModel(\n",
      "  (gat1): GATConv(47, 64, heads=1)\n",
      "  (gat2): GATConv(64, 64, heads=1)\n",
      "  (gat3): GATConv(64, 64, heads=1)\n",
      "  (pred_head): Linear(64, 20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO: training\n",
    "torch.manual_seed(33)\n",
    "\n",
    "model = TypeSeqModel(dataset, hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(train_loader:DataLoader, dev):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        data.to(dev)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # FIXME: change this to floats in Dataset if we need to stick with this\n",
    "        loss = criterion(out, data.y.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def eval(loader:DataLoader, dev):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data.to(dev)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # FIXME: change this to floats in Dataset if we need to stick with this\n",
    "        loss = criterion(out, data.y.to(torch.float32))\n",
    "        total_loss += loss.item()\n",
    "        pred_probabilities = F.softmax(out, dim=1)\n",
    "        correct += int((pred_probabilities.argmax(dim=1) == data.y.argmax(dim=1)).sum())\n",
    "    return correct/len(loader.dataset), total_loss/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device count: 4\n",
      "Device 0: Tesla M10\n",
      "Device 1: Tesla M10\n",
      "Device 2: Tesla M10\n",
      "Device 3: Tesla M10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(f'CUDA NOT AVAILABLE!')\n",
    "else:\n",
    "    print(f'Device count: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "CUDA_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Current device: cuda:0\n",
      "Current CUDA device: 0 (Tesla M10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0475, train accuracy = 3.75%\n",
      "Test loss = 0.0475, test accuracy = 2.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:24<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0186, train accuracy = 63.59%\n",
      "Test loss = 0.0177, test accuracy = 65.00%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "# 2m, 13.7s to get initial train/test accuracy on CPU\n",
    "# ~3m to get initial \" on GPU\n",
    "# 7m and counting without finishing first epoch using individual .to() methods...\n",
    "\n",
    "with torch.cuda.device(CUDA_DEVICE):\n",
    "    cuda_dev = torch.cuda.current_device()\n",
    "    device = f'cuda:{cuda_dev}' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'---------------')\n",
    "    print(f'Current device: {device}')\n",
    "    print(f'Current CUDA device: {cuda_dev} ({torch.cuda.get_device_name(cuda_dev)})')\n",
    "\n",
    "    # move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_acc, train_loss = eval(train_loader, device)\n",
    "    test_acc, test_loss = eval(test_loader, device)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc*100:,.2f}%')\n",
    "\n",
    "    for epoch in trange(1, 50):\n",
    "        train_one_epoch(train_loader, device)\n",
    "        train_acc, train_loss = eval(train_loader, device)\n",
    "        test_acc, test_loss = eval(test_loader, device)\n",
    "        # TODO: wandb...\n",
    "        torch.save(model, 'type_seq_model.pt')\n",
    "\n",
    "    train_acc, train_loss = eval(train_loader, device)\n",
    "    test_acc, test_loss = eval(test_loader, device)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc*100:,.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(b.x.shape[0]):\n",
    "#     print(decode_astnode(b.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rungid, bid, _, _, _ = dataset[0].varid\n",
    "# [(x.varid[0], x.varid[1]) for x in dataset[10000:10100]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m ss \u001b[38;5;241m=\u001b[39m Subset(dataset, indices)\n\u001b[1;32m      7\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(ss, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m [(x\u001b[38;5;241m.\u001b[39mvarid[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mvarid[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m loader]\n",
      "Cell \u001b[0;32mIn[177], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m ss \u001b[38;5;241m=\u001b[39m Subset(dataset, indices)\n\u001b[1;32m      7\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(ss, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m [(x\u001b[38;5;241m.\u001b[39mvarid[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mvarid[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m loader]\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/envs/phd/lib/python3.8/site-packages/torch_geometric/data/dataset.py:263\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 263\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    264\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "[(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "indices = [10000, 10001, 10002, 10003, 10004]\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "ss = Subset(dataset, indices)\n",
    "loader = DataLoader(ss, shuffle=True)\n",
    "[(x.varid[0][0], x.varid[0][1]) for x in loader]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
