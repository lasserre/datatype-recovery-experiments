{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "> PICK UP HERE: **implement model end-to-end ASAP using Dataset, saving model state every N epochs, etc.**\n",
    "- See todo list in OneNote\n",
    "- Once working, make the saving model state logic generic so I can use with any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input_params dict supplied but saved .json file also found\n",
      "input_params will be IGNORED in favor of saved .json file (model0_dataset/raw/input_params.json)\n"
     ]
    }
   ],
   "source": [
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/test_builds/astera.exp/rundata/run1',\n",
    "        '/home/cls0027/test_builds/coreutils.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "dataset = TypeSequenceDataset('model0_dataset', data_params, max_hops=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split test/train on BINARY boundary to prevent functions/vars from crossing\n",
    "# -> I have varid tuples, so I can get (rungid, binid) from that and use it to split\n",
    "#   - extract (idx, rungid, binid) for each data point and put into a dataframe\n",
    "#   - groupby/count dataframe by (rungid/binid) and come up with split that is close to target %\n",
    "#   - collect indices for each data point within each subset (test/train/valid)\n",
    "#   - quick/simple validate that all (rungid/binid) combos are unique to a subset\n",
    "#\n",
    "# [(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# tiny subset to start\n",
    "trainset_indices = list(range(12))\n",
    "testset_indices = list(range(100,112))\n",
    "\n",
    "train_set = Subset(dataset, trainset_indices)\n",
    "test_set = Subset(dataset, testset_indices)\n",
    "\n",
    "# TODO: we probably will need to ensure the dataset is a multiple of the chosen batch size\n",
    "# (maybe drop the last len(dataset)%BATCH_SIZE samples)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# EXAMPLE: the shuffled slice changes each time, but dataset[0] stays consistent\n",
    "# print(dataset[:10].shuffle()[0].varid)\n",
    "# print(dataset[0].varid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[59, 47], edge_index=[2, 112], y=[4, 20], varid=[4], batch=[59], ptr=[5])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "\n",
      "Step 2\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[53, 47], edge_index=[2, 106], y=[4, 20], varid=[4], batch=[53], ptr=[5])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3])\n",
      "\n",
      "Step 3\n",
      "Num graphs in batch: 4\n",
      "DataBatch(x=[88, 47], edge_index=[2, 170], y=[4, 20], varid=[4], batch=[88], ptr=[5])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step+1}')\n",
    "    print(f'Num graphs in batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print(data.batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[76, 47], edge_index=[2, 144], y=[4, 20], varid=[4], batch=[76], ptr=[5]),\n",
       " DataBatch(x=[66, 47], edge_index=[2, 126], y=[4, 20], varid=[4], batch=[66], ptr=[5]),\n",
       " DataBatch(x=[58, 47], edge_index=[2, 118], y=[4, 20], varid=[4], batch=[58], ptr=[5])]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader1 = DataLoader(train_set, batch_size=4)\n",
    "list(loader1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[11, 47], edge_index=[2, 20], y=[1, 20], varid=[1], batch=[11], ptr=[2]),\n",
       " DataBatch(x=[21, 47], edge_index=[2, 40], y=[1, 20], varid=[1], batch=[21], ptr=[2]),\n",
       " DataBatch(x=[17, 47], edge_index=[2, 32], y=[1, 20], varid=[1], batch=[17], ptr=[2]),\n",
       " DataBatch(x=[27, 47], edge_index=[2, 52], y=[1, 20], varid=[1], batch=[27], ptr=[2]),\n",
       " DataBatch(x=[14, 47], edge_index=[2, 26], y=[1, 20], varid=[1], batch=[14], ptr=[2]),\n",
       " DataBatch(x=[19, 47], edge_index=[2, 36], y=[1, 20], varid=[1], batch=[19], ptr=[2]),\n",
       " DataBatch(x=[23, 47], edge_index=[2, 46], y=[1, 20], varid=[1], batch=[23], ptr=[2]),\n",
       " DataBatch(x=[10, 47], edge_index=[2, 18], y=[1, 20], varid=[1], batch=[10], ptr=[2]),\n",
       " DataBatch(x=[5, 47], edge_index=[2, 16], y=[1, 20], varid=[1], batch=[5], ptr=[2]),\n",
       " DataBatch(x=[34, 47], edge_index=[2, 66], y=[1, 20], varid=[1], batch=[34], ptr=[2]),\n",
       " DataBatch(x=[6, 47], edge_index=[2, 10], y=[1, 20], varid=[1], batch=[6], ptr=[2]),\n",
       " DataBatch(x=[13, 47], edge_index=[2, 26], y=[1, 20], varid=[1], batch=[13], ptr=[2])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader2 = DataLoader(train_set, batch_size=1)\n",
    "list(loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 47])\n",
      "torch.Size([21, 47])\n",
      "torch.Size([17, 47])\n",
      "torch.Size([27, 47])\n",
      "torch.Size([14, 47])\n",
      "torch.Size([19, 47])\n",
      "torch.Size([23, 47])\n",
      "torch.Size([10, 47])\n",
      "torch.Size([5, 47])\n",
      "torch.Size([34, 47])\n",
      "torch.Size([6, 47])\n",
      "torch.Size([13, 47])\n"
     ]
    }
   ],
   "source": [
    "list(loader2)[0].x.shape\n",
    "for x in loader2:\n",
    "    print(x.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch\n",
      "torch.Size([76, 47]) torch.Size([2, 144])\n",
      "torch.Size([66, 47]) torch.Size([2, 126])\n",
      "torch.Size([58, 47]) torch.Size([2, 118])\n",
      "Non batch\n",
      "torch.Size([11, 47]) torch.Size([2, 20])\n",
      "torch.Size([21, 47]) torch.Size([2, 40])\n",
      "torch.Size([17, 47]) torch.Size([2, 32])\n",
      "torch.Size([27, 47]) torch.Size([2, 52])\n",
      "torch.Size([14, 47]) torch.Size([2, 26])\n",
      "torch.Size([19, 47]) torch.Size([2, 36])\n",
      "torch.Size([23, 47]) torch.Size([2, 46])\n",
      "torch.Size([10, 47]) torch.Size([2, 18])\n",
      "torch.Size([5, 47]) torch.Size([2, 16])\n",
      "torch.Size([34, 47]) torch.Size([2, 66])\n",
      "torch.Size([6, 47]) torch.Size([2, 10])\n",
      "torch.Size([13, 47]) torch.Size([2, 26])\n"
     ]
    }
   ],
   "source": [
    "print('Batch')\n",
    "for x in loader1:\n",
    "    print(x.x.shape, x.edge_index.shape)\n",
    "\n",
    "print('Non batch')\n",
    "for x in loader2:\n",
    "    print(x.x.shape, x.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.x shape: torch.Size([76, 47])\n",
      "b.batch shape: torch.Size([76])\n",
      "0 11\n",
      "1 21\n",
      "2 17\n",
      "3 27\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n"
     ]
    }
   ],
   "source": [
    "b = list(loader1)[0]\n",
    "print(f'b.x shape: {b.x.shape}')\n",
    "print(f'b.batch shape: {b.batch.shape}')\n",
    "\n",
    "for i in range(4):\n",
    "    print(i, len(b.batch[b.batch == i]))\n",
    "\n",
    "# TODO: need to verify I am collecting node 0 properly from each batch\n",
    "# - node 0 will always be a DeclRefExpr\n",
    "# - the model output prediction and ground truth y tensor for each sample\n",
    "#   corresponds to making a prediction on NODE 0\n",
    "# - I should be able to check this against a \"non-batched\" (and non-shuffled) data\n",
    "#   loader and make sure I get all the same answers\n",
    "\n",
    "b.x[0]\n",
    "(b.x[11]==b.x[0]).all()\n",
    "\n",
    "from datatype_recovery.models.dataset.encoding import decode_astnode\n",
    "\n",
    "print(decode_astnode(b.x[0]))\n",
    "print(decode_astnode(b.x[0+11]))\n",
    "print(decode_astnode(b.x[0+11+21]))\n",
    "print(decode_astnode(b.x[0+11+21+17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11\n",
      "32\n",
      "49\n",
      "\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(0)\n",
    "print(0+11)\n",
    "print(0+11+21)\n",
    "print(0+11+21+17)\n",
    "print()\n",
    "print(11+21+17+27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0_idx = (b.batch==3).nonzero(as_tuple=True)[0][0].item()\n",
    "node0_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeclRefExpr'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_astnode(b.x[node0_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Batches\n",
    "This code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[11, 47], edge_index=[2, 20], y=[1, 20], varid=[1], batch=[11], ptr=[2])\n",
      "DataBatch(x=[21, 47], edge_index=[2, 40], y=[1, 20], varid=[1], batch=[21], ptr=[2])\n",
      "DataBatch(x=[17, 47], edge_index=[2, 32], y=[1, 20], varid=[1], batch=[17], ptr=[2])\n",
      "DataBatch(x=[27, 47], edge_index=[2, 52], y=[1, 20], varid=[1], batch=[27], ptr=[2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 11, 32, 49]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or more elegantly...\n",
    "\n",
    "# def get_batch_\n",
    "batch_graphs = [(b.batch==i).nonzero(as_tuple=True) for i in range(BATCH_SIZE)]\n",
    "\n",
    "node0_indices = [x[0][0].item() for x in batch_graphs]\n",
    "node0_indices\n",
    "batch_graphs\n",
    "# batch shape: [76, 47]\n",
    "# - 76 = sum of nodes in all graphs within the batch = 11+21+17+27\n",
    "# - 47 = # node features\n",
    "batch_graphs\n",
    "# list of tensors, one tensor per graph within the batch\n",
    "# --> list of node_indexes, one per graph within the batch\n",
    "# each tensor holds a list of node indices for the nodes of that graph\n",
    "# (the indices index into b.x)\n",
    "# indices of nodes\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data.batch import DataBatch\n",
    "from typing import List\n",
    "\n",
    "def split_node_index_by_graph(data_batch:DataBatch, batch_size:int) -> List[torch.tensor]:\n",
    "    '''\n",
    "    Returns the node_index for each individual graph within the batch as a list of tensors.\n",
    "\n",
    "    Each list entry (node_index) is a tensor of node indices for an individual graph within the\n",
    "    batch, where each of the node indices will index into data_batch.x.\n",
    "    '''\n",
    "    return [(data_batch.batch==i).nonzero(as_tuple=True) for i in range(batch_size)]\n",
    "\n",
    "batch_graphs\n",
    "b.edge_index.shape\n",
    "\n",
    "# c = list(loader2)[:4]\n",
    "for x in list(loader2)[:4]:\n",
    "    print(x)\n",
    "\n",
    "# len(b.batch)\n",
    "# list(loader2)[3].edge_index\n",
    "# b.edge_index\n",
    "# b.batch[(b.batch==0).nonzero(as_tuple=True)]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# TODO: ok, here is the way to get node 0 indices\n",
    "# - edge_index should be able to take care of itself - we just pass it through, I\n",
    "#   don't think we need to do anything to it (it's correct, but won't have edges\n",
    "#   going between independent graphs of course)\n",
    "# ------------------------\n",
    "# TODO: PICK UP HERE and IMPLEMENT THE MODEL!!!\n",
    "# ------------------------\n",
    "# - use node0_indices to pull target node embeddings, passing into final layer (linear for now?)\n",
    "# - return prediction...finish building training function\n",
    "# - INTEGRATE WANDB!!\n",
    "# ------------------------\n",
    "node_index_by_graph = split_node_index_by_graph(b, BATCH_SIZE)\n",
    "node0_indices = [x[0][0].item() for x in node_index_by_graph]\n",
    "node0_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b.x[node0_indices] == b.x[:4]).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeclRefExpr\n",
      "BinaryOperator\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "BinaryOperator\n",
      "ParenExpr\n",
      "CStyleCastExpr\n",
      "IntegerLiteral\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "BinaryOperator\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "StringLiteral\n",
      "BinaryOperator\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "StringLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "StringLiteral\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "BinaryOperator\n",
      "ParenExpr\n",
      "DeclRefExpr\n",
      "CallExpr\n",
      "BinaryOperator\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "UnaryOperator\n",
      "CStyleCastExpr\n",
      "UnaryOperator\n",
      "CStyleCastExpr\n",
      "DeclRefExpr\n",
      "DeclRefExpr\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "BinaryOperator\n",
      "IntegerLiteral\n",
      "CallExpr\n",
      "DeclRefExpr\n",
      "IntegerLiteral\n",
      "UnaryOperator\n",
      "DeclRefExpr\n",
      "UnaryOperator\n",
      "DeclRefExpr\n",
      "IntegerLiteral\n",
      "IntegerLiteral\n",
      "IntegerLiteral\n",
      "IntegerLiteral\n",
      "IntegerLiteral\n"
     ]
    }
   ],
   "source": [
    "for i in range(b.x.shape[0]):\n",
    "    print(decode_astnode(b.x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n",
      "Non batch\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "print('Batch')\n",
    "for x in loader1:\n",
    "    print(x.y.shape)\n",
    "\n",
    "print('Non batch')\n",
    "for x in loader2:\n",
    "    print(x.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class TypeSeqModel(torch.nn.Module):\n",
    "    def __init__(self, dataset:Dataset, hidden_channels:int):\n",
    "        super(TypeSeqModel, self).__init__()\n",
    "\n",
    "        self.gat1 = GATConv(dataset.num_node_features, hidden_channels)\n",
    "        self.gat2 = GATConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training\n",
    "\n",
    "torch.manual_seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44021"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TEMP: just split randomly for now\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 81.0), (1, 81.0), (1, 81.0), (1, 81.0), (1, 82.0)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rungid, bid, _, _, _ = dataset[0].varid\n",
    "[(x.varid[0], x.varid[1]) for x in dataset[10000:10100]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, x.varid[0], x.varid[1]) for i, x in enumerate(dataset[10000:10100])][:5]\n",
    "\n",
    "indices = [10000, 10001, 10002, 10003, 10004]\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "ss = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(x.varid[0], x.varid[1]) for x in ss.shuffle()]\n",
    "loader = DataLoader(ss, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 81.0), (1, 82.0), (1, 81.0), (1, 81.0), (1, 81.0)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.varid[0][0], x.varid[0][1]) for x in loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "        ... \n",
       "11580    116\n",
       "11581    116\n",
       "11582    116\n",
       "11583    116\n",
       "11584    116\n",
       "Name: BinaryId, Length: 11585, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/binaries.csv')).BinaryId\n",
    "df = pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/locals.csv'))\n",
    "# df.BinaryId = df.BinaryId.astype('int64')\n",
    "# df.loc[:,'NEW_COL'] = pd.Series([1e3]*len(df), dtype='int64')\n",
    "df.BinaryId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1000\n",
       "1        1000\n",
       "2        1000\n",
       "3        1000\n",
       "4        1000\n",
       "         ... \n",
       "11580    1000\n",
       "11581    1000\n",
       "11582    1000\n",
       "11583    1000\n",
       "11584    1000\n",
       "Name: NEW_COL, Length: 11585, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.NEW_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      47\n",
       "1      47\n",
       "2      47\n",
       "3      47\n",
       "4      47\n",
       "       ..\n",
       "101    47\n",
       "102    47\n",
       "103    47\n",
       "104    47\n",
       "105    47\n",
       "Name: BinaryId, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path('/home/cls0027/test_builds/coreutils.exp/rundata/run1/47.pathchk/functions.csv'))\n",
    "df.BinaryId"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
