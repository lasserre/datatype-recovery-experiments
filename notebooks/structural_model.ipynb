{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: input_params dict supplied but saved .json file also found\n",
      "input_params will be IGNORED in favor of saved .json file (trainset_astera/raw/input_params.json)\n",
      "Warning: only computing accuracy on fixed-length size of 4\n",
      "TODO: compute accuracy based on raw type sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span>Training with small subset of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">096</span> samples<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0mTraining with small subset of \u001b[1;36m4\u001b[0m,\u001b[1;36m096\u001b[0m samples\u001b[92m ───────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 2,816 samples (68.75%)\n",
      "Test set: 1,280 samples (31.25%)\n",
      "Batch size: 64\n",
      "Total usable dataset size (batch-aligned): 4,096\n",
      "Loss due to batch alignment: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datatype_recovery.models.dataset import TypeSequenceDataset, InMemTypeSequenceDataset\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from datatype_recovery.models.dataset.encoding import ToFixedLengthTypeSeq\n",
    "\n",
    "# model\n",
    "HIDDEN_CHANNELS = 128\n",
    "MAX_SEQ_LEN = 4\n",
    "MAX_HOPS = 3\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_SPLIT = 0.7\n",
    "NUM_EPOCHS = 5\n",
    "SHUFFLE = True\n",
    "PIN_MEMORY=False\n",
    "\n",
    "# optimizer\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "torch.manual_seed(233)   # deterministic hopefully? lol\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/exp_builds/astera.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "\n",
    "slow_dataset = TypeSequenceDataset('trainset_astera', data_params, max_hops=MAX_HOPS)\n",
    "dataset = InMemTypeSequenceDataset(slow_dataset)\n",
    "\n",
    "transform = ToFixedLengthTypeSeq(MAX_SEQ_LEN)\n",
    "dataset.transform = transform   # apply transform here so we can remove it if desired\n",
    "print(f'Warning: only computing accuracy on fixed-length size of {MAX_SEQ_LEN}')\n",
    "print(f'TODO: compute accuracy based on raw type sequence')\n",
    "\n",
    "import rich\n",
    "console = rich.console.Console()\n",
    "\n",
    "OVERFIT_SIZE = 4096\n",
    "\n",
    "if OVERFIT_SIZE is not None:\n",
    "    dataset = dataset[:OVERFIT_SIZE]    # TEMP: overfit on tiny subset\n",
    "    console.rule(f'Training with small subset of {OVERFIT_SIZE:,} samples')\n",
    "\n",
    "# divide into train/test sets - aligning to batch size\n",
    "train_size = int(len(dataset)*TRAIN_SPLIT/BATCH_SIZE) * BATCH_SIZE\n",
    "test_size = int((len(dataset) - train_size)/BATCH_SIZE) * BATCH_SIZE\n",
    "\n",
    "train_indices = [int(x) for x in torch.randperm(len(dataset))[:train_size]]\n",
    "test_indices = set(range(1024)) - set(train_indices)\n",
    "test_indices = list(test_indices)[:test_size]   # align to batch size\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "test_set = Subset(dataset, range(len(train_set), len(train_set)+test_size))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "\n",
    "total_usable = len(train_set)+len(test_set)\n",
    "non_batch_aligned = len(dataset)-total_usable\n",
    "print()\n",
    "print(f'Train set: {len(train_set):,} samples ({len(train_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Test set: {len(test_set):,} samples ({len(test_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Total usable dataset size (batch-aligned): {total_usable:,}')\n",
    "print(f'Loss due to batch alignment: {non_batch_aligned:,} ({non_batch_aligned/len(dataset)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainset_astera'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(train_loader.dataset.dataset.root).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datatype_recovery.models.dataset.encoding import *\n",
    "[decode_typeseq(dataset[i].y) for i in range(10)]\n",
    "decode_typeseq(dataset[4].y)\n",
    "typeseq_name_to_id()['COMP']\n",
    "int(dataset[4].y.argmax(dim=1)[0]) == 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'experiment_runs': ['/home/cls0027/exp_builds/astera.exp/rundata/run1'],'copy_data': False, 'drop_component': True}\n",
    "testing = TypeSequenceDataset('DEBUG', params, max_hops=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatype_recovery.models.dataset.encoding import decode_typeseq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# TODO: PLOT DATASET BALANCE (as above) BUT USE ORIGINAL DATA FOR SPEED\n",
    "##########################################################################\n",
    "# --> I want to be able to hand it a Dataset object though, so I know I'm\n",
    "#     only computing results based on THIS DATA\n",
    "# --> Extract varids for the Dataset/DataLoader\n",
    "# --> for each varid, pull its pandas data from the source csvs (or copied ones)\n",
    "# --> Create a new dataframe from this with the columns above\n",
    "#\n",
    "# ...then generate the plot\n",
    "\n",
    "# NOTE: this works great, but is too slow! port this code over...\n",
    "# -----------\n",
    "# train_typesequences = [decode_typeseq(sample.y) for sample in train_set]\n",
    "# test_typesequences = [decode_typeseq(sample.y) for sample in test_set]\n",
    "\n",
    "# train_df = pd.DataFrame({\n",
    "#     'TypeSeq': train_typesequences,\n",
    "#     'TypeSeqLen': [len(x) for x in train_typesequences],\n",
    "#     'FirstType': [x[0] for x in train_typesequences],\n",
    "#     'Varid': [sample.varid for sample in train_set],\n",
    "#     'Split': ['Train']*len(train_set),\n",
    "#     'Train': [True]*len(train_set),\n",
    "# })\n",
    "\n",
    "# test_df = pd.DataFrame({\n",
    "#     'TypeSeq': test_typesequences,\n",
    "#     'TypeSeqLen': [len(x) for x in test_typesequences],\n",
    "#     'FirstType': [x[0] for x in test_typesequences],\n",
    "#     'Varid': [sample.varid for sample in test_set],\n",
    "#     'Split': ['Test']*len(test_set),\n",
    "#     'Test': [True]*len(test_set),\n",
    "# })\n",
    "\n",
    "# df = pd.concat([train_df, test_df])\n",
    "\n",
    "# df.groupby(['FirstType'])[['Train','Test']].count().sort_values('Train') \\\n",
    "#     .plot(kind='barh')\n",
    "\n",
    "# dataset_classes = df.groupby(['FirstType'])[['Train','Test']].count()\n",
    "# dataset_classes.loc[:,'TrainPcnt'] = dataset_classes.Train/dataset_classes.Train.sum()*100\n",
    "# dataset_classes.loc[:,'TestPcnt'] = dataset_classes.Test/dataset_classes.Test.sum()*100\n",
    "# dataset_classes\n",
    "# df.TypeSeqLen.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device count: 4\n",
      "Device 0: Tesla M10\n",
      "Device 1: Tesla M10\n",
      "Device 2: Tesla M10\n",
      "Device 3: Tesla M10\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(f'CUDA NOT AVAILABLE!')\n",
    "else:\n",
    "    print(f'Device count: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "CUDA_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructuralTypeSeqModel(\n",
      "  (gat_layers): ModuleList(\n",
      "    (0): GATConv(31, 128, heads=1)\n",
      "    (1-2): 2 x GATConv(128, 128, heads=1)\n",
      "  )\n",
      "  (pred_head): Linear(128, 88, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcstew33\u001b[0m (\u001b[33muah-phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cls0027/dev/datatype-recovery-experiments/notebooks/wandb/run-20240206_211649-a5r5a4q5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uah-phd/StructuralModel/runs/a5r5a4q5' target=\"_blank\">radiant-shadow-10</a></strong> to <a href='https://wandb.ai/uah-phd/StructuralModel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uah-phd/StructuralModel' target=\"_blank\">https://wandb.ai/uah-phd/StructuralModel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uah-phd/StructuralModel/runs/a5r5a4q5' target=\"_blank\">https://wandb.ai/uah-phd/StructuralModel/runs/a5r5a4q5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 epochs\n",
      "---------------\n",
      "Current device: cuda:0\n",
      "Current CUDA device: 0 (Tesla M10)\n",
      "Computing initial accuracy/loss...\n",
      "Train loss = 0.0475, train accuracy = 0.00%\n",
      "Test loss = 0.0476, test accuracy = 0.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bcd687cd5a40d4b72b2dd4e1085f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "globals must be a real dict; try eval(expr, {}, mapping)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m\n\u001b[1;32m     88\u001b[0m         wandb\u001b[38;5;241m.\u001b[39malert(\n\u001b[1;32m     89\u001b[0m             title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m             text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached test accuracy of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc_bin\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     91\u001b[0m             level\u001b[38;5;241m=\u001b[39mAlertLevel\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m         curr_acc_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 95\u001b[0m train_acc_bin, train_acc_weight, train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m test_acc_bin, test_acc_weight, test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(test_loader, device)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc_bin\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: globals must be a real dict; try eval(expr, {}, mapping)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "from datatype_recovery.models.structural_model import StructuralTypeSeqModel\n",
    "from datatype_recovery.models.dataset.encoding import *\n",
    "from datatype_recovery.models.training import *\n",
    "\n",
    "model_path = Path.cwd()/'structural_model.pt'\n",
    "model = StructuralTypeSeqModel(dataset, MAX_SEQ_LEN, HIDDEN_CHANNELS, num_hops=MAX_HOPS)\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f'Loading existing model state @ {model_path}')\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "dataset_names = ','.join([Path(x).parent.parent.stem for x in data_params['experiment_runs']])\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"StructuralModel\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LEARN_RATE,\n",
    "        \"architecture\": \"GATConv\",\n",
    "        'max_hops': MAX_HOPS,\n",
    "        'max_seq_len': MAX_SEQ_LEN,\n",
    "        'hidden_channels': HIDDEN_CHANNELS,\n",
    "        \"dataset\": dataset_names,\n",
    "        'dataset_size': len(dataset),\n",
    "        'train_split': TRAIN_SPLIT,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "    }\n",
    ")\n",
    "\n",
    "# once I fix this above...\n",
    "# wandb.log({\n",
    "#     'Dataset/classes': wandb.Table(dataframe=dataset_classes \\\n",
    "#                         .sort_values('Train',ascending=False) \\\n",
    "#                         .reset_index()),\n",
    "#     'Dataset/plot': wandb.Image(\n",
    "#         dataset_classes[['TrainPcnt','TestPcnt']].sort_values('TrainPcnt').plot(kind='barh')\n",
    "#     ),\n",
    "# })\n",
    "\n",
    "# with torch.cuda.device(CUDA_DEVICE):\n",
    "cuda_dev = torch.cuda.current_device()\n",
    "device = f'cuda:{cuda_dev}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Training for {NUM_EPOCHS} epochs')\n",
    "\n",
    "notify_acc_levels = [0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 2]     # the 2 is to stop notifications :)\n",
    "curr_acc_idx = 0\n",
    "\n",
    "with TrainContext(model, device, optimizer, criterion, MAX_SEQ_LEN) as ctx:\n",
    "\n",
    "    print(f'Computing initial accuracy/loss...')\n",
    "    train_acc_bin, train_acc_weight, train_loss = ctx.eval(train_loader)\n",
    "    test_acc_bin, test_acc_weight, test_loss = ctx.eval(test_loader)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc_bin*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc_bin*100:,.2f}%')\n",
    "\n",
    "    for epoch in trange(NUM_EPOCHS):\n",
    "        ctx.train_one_epoch(train_loader)\n",
    "        train_acc_bin, train_acc_weight, train_loss = ctx.eval(train_loader)\n",
    "        test_acc_bin, test_acc_weight, test_loss = ctx.eval(test_loader)\n",
    "        wandb.log({\n",
    "            'train/loss': train_loss,\n",
    "            'train/acc': train_acc_bin,\n",
    "            'train/acc_weighted': train_acc_weight,\n",
    "            'test/loss': test_loss,\n",
    "            'test/acc': test_acc_bin,\n",
    "            'test/acc_weighted': test_acc_weight,\n",
    "        })\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "        if test_acc_bin >= notify_acc_levels[curr_acc_idx]:\n",
    "            wandb.alert(\n",
    "                title='Test accuracy',\n",
    "                text=f'Reached test accuracy of {test_acc_bin*100:.2f}%',\n",
    "                level=AlertLevel.INFO,\n",
    "            )\n",
    "            curr_acc_idx += 1\n",
    "\n",
    "    train_acc_bin, train_acc_weight, train_loss = eval(train_loader, device)\n",
    "    test_acc_bin, test_acc_weight, test_loss = eval(test_loader, device)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc_bin*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc_bin*100:,.2f}%')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
