{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datatype_recovery.models.dataset import TypeSequenceDataset\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from datatype_recovery.models.dataset.encoding import ToFixedLengthTypeSeq\n",
    "\n",
    "# model\n",
    "HIDDEN_CHANNELS = 128\n",
    "MAX_SEQ_LEN = 4\n",
    "MAX_HOPS = 3\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_SPLIT = 0.7\n",
    "NUM_EPOCHS = 500\n",
    "SHUFFLE = True\n",
    "PIN_MEMORY=False\n",
    "\n",
    "# optimizer\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "torch.manual_seed(233)   # deterministic hopefully? lol\n",
    "\n",
    "data_params = {\n",
    "    'experiment_runs': [\n",
    "        '/home/cls0027/exp_builds/astera.exp/rundata/run1',\n",
    "    ],\n",
    "    'copy_data': False,\n",
    "}\n",
    "\n",
    "dataset = TypeSequenceDataset('trainset_astera', data_params, max_hops=MAX_HOPS)\n",
    "transform = ToFixedLengthTypeSeq(MAX_SEQ_LEN)\n",
    "dataset.transform = transform   # apply transform here so we can remove it if desired\n",
    "print(f'Warning: only computing accuracy on fixed-length size of {MAX_SEQ_LEN}')\n",
    "print(f'TODO: compute accuracy based on raw type sequence')\n",
    "\n",
    "import rich\n",
    "console = rich.console.Console()\n",
    "\n",
    "OVERFIT_SIZE = 4096\n",
    "\n",
    "if OVERFIT_SIZE is not None:\n",
    "    dataset = dataset[:OVERFIT_SIZE]    # TEMP: overfit on tiny subset\n",
    "    console.rule(f'Training with small subset of {OVERFIT_SIZE:,} samples')\n",
    "\n",
    "# divide into train/test sets - aligning to batch size\n",
    "train_size = int(len(dataset)*TRAIN_SPLIT/BATCH_SIZE) * BATCH_SIZE\n",
    "test_size = int((len(dataset) - train_size)/BATCH_SIZE) * BATCH_SIZE\n",
    "\n",
    "train_indices = [int(x) for x in torch.randperm(len(dataset))[:train_size]]\n",
    "test_indices = set(range(1024)) - set(train_indices)\n",
    "test_indices = list(test_indices)[:test_size]   # align to batch size\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "test_set = Subset(dataset, range(len(train_set), len(train_set)+test_size))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "\n",
    "total_usable = len(train_set)+len(test_set)\n",
    "non_batch_aligned = len(dataset)-total_usable\n",
    "print()\n",
    "print(f'Train set: {len(train_set):,} samples ({len(train_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Test set: {len(test_set):,} samples ({len(test_set)/len(dataset)*100:.2f}%)')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Total usable dataset size (batch-aligned): {total_usable:,}')\n",
    "print(f'Loss due to batch alignment: {non_batch_aligned:,} ({non_batch_aligned/len(dataset)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatype_recovery.models.dataset.encoding import decode_typeseq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# TODO: PLOT DATASET BALANCE (as above) BUT USE ORIGINAL DATA FOR SPEED\n",
    "##########################################################################\n",
    "# --> I want to be able to hand it a Dataset object though, so I know I'm\n",
    "#     only computing results based on THIS DATA\n",
    "# --> Extract varids for the Dataset/DataLoader\n",
    "# --> for each varid, pull its pandas data from the source csvs (or copied ones)\n",
    "# --> Create a new dataframe from this with the columns above\n",
    "#\n",
    "# ...then generate the plot\n",
    "\n",
    "# NOTE: this works great, but is too slow! port this code over...\n",
    "# -----------\n",
    "# train_typesequences = [decode_typeseq(sample.y) for sample in train_set]\n",
    "# test_typesequences = [decode_typeseq(sample.y) for sample in test_set]\n",
    "\n",
    "# train_df = pd.DataFrame({\n",
    "#     'TypeSeq': train_typesequences,\n",
    "#     'TypeSeqLen': [len(x) for x in train_typesequences],\n",
    "#     'FirstType': [x[0] for x in train_typesequences],\n",
    "#     'Varid': [sample.varid for sample in train_set],\n",
    "#     'Split': ['Train']*len(train_set),\n",
    "#     'Train': [True]*len(train_set),\n",
    "# })\n",
    "\n",
    "# test_df = pd.DataFrame({\n",
    "#     'TypeSeq': test_typesequences,\n",
    "#     'TypeSeqLen': [len(x) for x in test_typesequences],\n",
    "#     'FirstType': [x[0] for x in test_typesequences],\n",
    "#     'Varid': [sample.varid for sample in test_set],\n",
    "#     'Split': ['Test']*len(test_set),\n",
    "#     'Test': [True]*len(test_set),\n",
    "# })\n",
    "\n",
    "# df = pd.concat([train_df, test_df])\n",
    "\n",
    "# df.groupby(['FirstType'])[['Train','Test']].count().sort_values('Train') \\\n",
    "#     .plot(kind='barh')\n",
    "\n",
    "# dataset_classes = df.groupby(['FirstType'])[['Train','Test']].count()\n",
    "# dataset_classes.loc[:,'TrainPcnt'] = dataset_classes.Train/dataset_classes.Train.sum()*100\n",
    "# dataset_classes.loc[:,'TestPcnt'] = dataset_classes.Test/dataset_classes.Test.sum()*100\n",
    "# dataset_classes\n",
    "# df.TypeSeqLen.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(f'CUDA NOT AVAILABLE!')\n",
    "else:\n",
    "    print(f'Device count: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "CUDA_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "from datatype_recovery.models.structural_model import StructuralTypeSeqModel\n",
    "from datatype_recovery.models.dataset.encoding import *\n",
    "from datatype_recovery.models.training import *\n",
    "\n",
    "model_path = Path.cwd()/'structural_model.pt'\n",
    "model = StructuralTypeSeqModel(dataset, MAX_SEQ_LEN, HIDDEN_CHANNELS, num_hops=MAX_HOPS)\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f'Loading existing model state @ {model_path}')\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "dataset_names = ','.join([Path(x).parent.parent.stem for x in data_params['experiment_runs']])\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"StructuralModel\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LEARN_RATE,\n",
    "        \"architecture\": \"GATConv\",\n",
    "        'max_hops': MAX_HOPS,\n",
    "        'max_seq_len': MAX_SEQ_LEN,\n",
    "        'hidden_channels': HIDDEN_CHANNELS,\n",
    "        \"dataset\": dataset_names,\n",
    "        'dataset_size': len(dataset),\n",
    "        'train_split': TRAIN_SPLIT,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "    }\n",
    ")\n",
    "\n",
    "# once I fix this above...\n",
    "# wandb.log({\n",
    "#     'Dataset/classes': wandb.Table(dataframe=dataset_classes \\\n",
    "#                         .sort_values('Train',ascending=False) \\\n",
    "#                         .reset_index()),\n",
    "#     'Dataset/plot': wandb.Image(\n",
    "#         dataset_classes[['TrainPcnt','TestPcnt']].sort_values('TrainPcnt').plot(kind='barh')\n",
    "#     ),\n",
    "# })\n",
    "\n",
    "# with torch.cuda.device(CUDA_DEVICE):\n",
    "cuda_dev = torch.cuda.current_device()\n",
    "device = f'cuda:{cuda_dev}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Training for {NUM_EPOCHS} epochs')\n",
    "\n",
    "with TrainContext(model, device, optimizer, criterion, MAX_SEQ_LEN) as ctx:\n",
    "\n",
    "    train_acc_bin, train_acc_weight, train_loss = ctx.eval(train_loader)\n",
    "    test_acc_bin, test_acc_weight, test_loss = ctx.eval(test_loader)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc_bin*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc_bin*100:,.2f}%')\n",
    "\n",
    "    for epoch in trange(NUM_EPOCHS):\n",
    "        ctx.train_one_epoch(train_loader)\n",
    "        train_acc_bin, train_acc_weight, train_loss = ctx.eval(train_loader)\n",
    "        test_acc_bin, test_acc_weight, test_loss = ctx.eval(test_loader)\n",
    "        wandb.log({\n",
    "            'train/loss': train_loss,\n",
    "            'train/acc': train_acc_bin,\n",
    "            'train/acc_weighted': train_acc_weight,\n",
    "            'test/loss': test_loss,\n",
    "            'test/acc': test_acc_bin,\n",
    "            'test/acc_weighted': test_acc_weight,\n",
    "        })\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "    train_acc_bin, train_acc_weight, train_loss = eval(train_loader, device)\n",
    "    test_acc_bin, test_acc_weight, test_loss = eval(test_loader, device)\n",
    "    print(f'Train loss = {train_loss:.4f}, train accuracy = {train_acc_bin*100:,.2f}%')\n",
    "    print(f'Test loss = {test_loss:.4f}, test accuracy = {test_acc_bin*100:,.2f}%')\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
